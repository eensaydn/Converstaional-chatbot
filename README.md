# ðŸ§  Conversational RAG with History-Aware Question Reformulation (LangChain + Groq)

This repository demonstrates a professional implementation of a **Conversational Retrieval-Augmented Generation (RAG)** system using **LangChain**, powered by **Groq (LLaMA 3)** for fast inference and enhanced with **chat history awareness**.

## ðŸš€ Features

- ðŸ’¬ Multi-turn conversational RAG system  
- ðŸ§  Contextual question reformulation using previous chat history  
- ðŸ” Semantic search with HuggingFace embeddings and Chroma vector store  
- âš¡ Powered by Groq API for ultra-fast LLM inference  
- ðŸ§¾ Memory support with per-session history management  
- ðŸ”§ Modular and easy to extend with your own data/documents  

---

## ðŸ§± Tech Stack

| Component         | Technology Used                      |
|------------------|--------------------------------------|
| Language Model    | Groq (LLaMA3-8B) via `langchain_groq` |
| Vector DB         | Chroma                              |
| Embedding Model   | all-MiniLM-L6-v2 (`langchain_huggingface`) |
| Framework         | LangChain                           |
| Chat History Store| In-memory (extendable to Redis etc.)|

---

## ðŸ§© Architecture

```mermaid
flowchart TD
    A[User Question] --> B[Chat History + Question]
    B --> C[Rephrase Question with LLM]
    C --> D[Retriever Chroma + Embedding]
    D --> E[Relevant Documents]
    E --> F[LLM Answer using Stuff Chain]
    F --> G[Response + Updated History]

project/
â”œâ”€â”€ app.py                         # Main application logic
â”œâ”€â”€ .env                           # API keys and configuration
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ requirements.txt               # Python dependencies

